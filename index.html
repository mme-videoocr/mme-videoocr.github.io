<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="MME-VideoOCR">
  <meta property="og:title" content="MME-VideoOCR"/>
  <meta property="og:description" content="MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios"/>
  <meta property="og:url" content="https://mme-videoocr.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" /> -->
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<style>
  .tight-figure {
    margin-bottom: 0; /* 去除图像下方空隙 */
    margin-top: 0; /* 去除图像上方空隙 */
    margin-left: auto;
    margin-right: auto;
    display: block;
  }

  .tight-text {
    margin-top: 0.5rem; /* 缩小图像和段落之间的空隙 */
  }

  .is-80-percent {
    width: 110%;
    margin-left: auto;
    margin-right: auto;
    padding-left: 1rem;
    padding-right: 1rem;
  }

  .is-small-padding {
  padding-top: 0rem;
  padding-bottom: 0rem;
  margin-top: 0rem;
  margin-bottom: 0rem;
}
</style>


  <section class="section is-small-padding">
    <div class="hero-body">
      <div class="container"><style></style>
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <style>
                  .author-block sup {
                    font-weight: bold;
                  }
                    .color-1 { color: #1f77b4; } /* Peking University */
                    .color-2 { color: #ff7f0e; } /* Tsinghua University */
                    .color-3 { color: #2ca02c; } /* CASIA */
                    .color-4 { color: #7f7f7f; } /* CUHKSZ */
                    .color-5 { color: #9467bd; } /* NTU */
                    .color-6 { color: #8c564b; } /* Nanjing University */
                    .color-7 { color: #e377c2; } /* XJTU */
                    .color-8 { color: #d62728; } /* Kuaishou Technology */
                  </style>

                  <span class="author-block">Yang Shi<sup class="color-1">1</sup><sup>,</sup><sup class="color-8">8</sup><sup>*</sup>,</span>
                  <span class="author-block">Huanqian Wang<sup class="color-2">2</sup><sup>*</sup>,</span>
                  <span class="author-block">Wulin Xie<sup class="color-3">3</sup><sup>*</sup>,</span>
                  <span class="author-block">Huanyao Zhang<sup class="color-1">1</sup><sup>*</sup>,</span>
                  <span class="author-block">Lijie Zhao<sup class="color-4">4</sup><sup>*</sup>,</span>
                  <span class="author-block">Yi-Fan Zhang<sup class="color-3">3</sup><sup>*</sup><sup>†</sup>,</span><br>
                  <span class="author-block">Xinfeng Li<sup class="color-5">5</sup>,</span>
                  <span class="author-block">Chaoyou Fu<sup class="color-6">6</sup>,</span>
                  <span class="author-block">Zhuoer Wen<sup class="color-1">1</sup>,</span>
                  <span class="author-block">Wenting Liu<sup class="color-1">1</sup>,</span>
                  <span class="author-block">Zhuoran Zhang<sup class="color-1">1</sup>,</span>
                  <span class="author-block">Xinlong Chen<sup class="color-3">3</sup>,</span>
                  <span class="author-block">Bohan Zeng<sup class="color-1">1</sup>,</span><br>
                  <span class="author-block">Sihan Yang<sup class="color-7">7</sup>,</span>
                  <span class="author-block">Yuanxing Zhang<sup class="color-8">8</sup><sup>‡</sup>,</span>
                  <span class="author-block">Pengfei Wan<sup class="color-8">8</sup>,</span>
                  <span class="author-block">Haotian Wang<sup class="color-2">2</sup><sup>†</sup>,</span>
                  <span class="author-block">Wenjing Yang<sup>†</sup></span><br>

                <div class="is-size-5 publication-authors"></div>
                <div class="is-size-5 publication-authors">
                  <span class="author-block">
                  <sup class="color-1">1</sup>PKU, 
                  <sup class="color-2">2</sup>THU, 
                  <sup class="color-3">3</sup>CASIA, 
                  <sup class="color-4">4</sup>CUHKSZ, 
                  <sup class="color-5">5</sup>NTU, <br>
                  <sup class="color-6">6</sup>NJU, 
                  <sup class="color-7">7</sup>XJTU, 
                  <sup class="color-8">8</sup>Kuaishou<br>
                  </span>
                </div>
                <span class="eql-cntrb">
                  <sup>*</sup>Core Contributor <sup>†</sup>Corresponding Author <sup>‡</sup>Project Lead</small>
                </span>
              </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/DogNeverSleep/MME-VideoOCR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Huggingface Dataset -->
                <!-- Huggingface Dataset -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/DogNeverSleep/MME-VideoOCR_Dataset" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper Teaser -->
<section class="section is-small-padding">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="section is-80-percent">
        <h2 class="title is-3">Teaser</h2>

        <figure class="is-tight-figure">
            <img src="static/images/teaser.png" alt="Teaser Image" style="max-width: 100%; height: auto;">
        </figure>
        <p class="tight-text" style="text-align: left;">
                <p><strong>An example in MME-VideoOCR</strong>. The task requires the MLLM to first recognize the textual information distributed across multiple video frames, and then to perform semantic understanding and reasoning over the extracted text to accurately determine the correct answer. The correct information is marked in <span style="color: #0070C0;">blue</span>, while misleading information is marked in <span style="color: #C00000;">red</span>.</p>
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End paper teaser -->

<!-- Visualization -->
<section class="section is-small-padding">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="section is-80-percent">
        <h2 class="title is-3">Visualization</h2>

        <figure class="is-tight-figure">
          <img src="static/images/visualization.png" alt="Teaser Image" style="max-width: 100%; height: auto;">
        </figure>

        <!-- 修改后的文字部分 -->
        <p style="text-align: left; white-space: normal; max-width: 95%; margin: 0 auto;">
          <strong>Example videos and their annotated questions from the MME-VideoOCR benchmark</strong>, encompassing 25 tasks across 10 categories. Each task is designed to evaluate models' capabilities in various aspects such as text recognition, localization, reasoning, and comprehensive video understanding. The figure displays representative video samples and their corresponding questions.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Visualization -->

<style>
  /* Table specific styles - Ensure these don't conflict with your main CSS */
  #EvaluationResults table {
    width: 100%; /* Table fills its container */
    border-collapse: collapse;
    font-family: sans-serif;
    border: 1px solid #ddd;
    margin-bottom: 20px; /* Add some space below the table */
  }
  #EvaluationResults caption {
    font-weight: bold;
    margin-bottom: 15px; /* Increased space */
    text-align: left;
    font-size: 1.1em; /* Slightly larger caption */
  }
  #EvaluationResults th, 
  #EvaluationResults td {
    border: 1px solid #ddd;
    padding: 8px;
    text-align: center;
    vertical-align: middle; /* Ensure vertical alignment */
  }
  #EvaluationResults th {
    background-color: #f2f2f2;
  }
  #EvaluationResults td:first-child, 
  #EvaluationResults th:first-child {
      text-align: left;
  }
  #EvaluationResults .highest {
    background-color: #F08080; /* LightCoral */
    color: black;
  }
  #EvaluationResults .second-highest {
    text-decoration: underline;
  }
  #EvaluationResults .section-header {
    background-color: #f9f9f9;
    color: gray;
    font-family: monospace;
    text-align: center;
    font-weight: bold; /* Make section headers bold */
  }
  #EvaluationResults code {
      font-family: monospace;
      background-color: #eee; /* Style code slightly */
      padding: 2px 4px;
      border-radius: 3px;
  }
  /* Optional: Ensure h2 has some space below it if 'content' class doesn't handle it */
  #EvaluationResults .title {
      margin-bottom: 1.5rem;
  }
</style>

<section class="section" id="EvaluationResults">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">Leaderboard</h2>
    
    <table>
      <caption>
        <b>Evaluation results on MME-VideoOCR.</b>
        <code>TR</code> denotes Text Recognition,
        <code>VTQA</code> Visual Text QA,
        <code>TG</code> Text Grounding,
        <code>AR</code> Attribute Recognition,
        <code>CDT</code> Change Detection & Tracking,
        <code>STP</code> Special Text Parsing,
        <code>CFTU</code> Cross-Frame Text Understanding,
        <code>TBR</code> Text-Based Reasoning,
        <code>TBVU</code> Text-Based Video Understanding, and
        <code>RVT</code> Robust Video Testing.
        The highest accuracy of each task is in <span style="background-color:#F08080; padding: 2px;">red</span>,
        and the second highest is <u>underlined</u>.
      </caption>
      <thead>
        <tr>
          <th>Model</th>
          <th>Size</th>
          <th>TR</th>
          <th>VTQA</th>
          <th>TG</th>
          <th>AR</th>
          <th>CDT</th>
          <th>STP</th>
          <th>CFTU</th>
          <th>TBR</th>
          <th>TBVU</th>
          <th>RVT</th>
          <th>Total</th>
        </tr>
      </thead>
      <tbody>
        <tr>
            <th colspan="13" class="section-header has-text-centered">Closed-source MLLMs</th>
        </tr>
        <tr>
          <td>Gemini-1.5 Pro</td>
          <td>-</td>
          <td>76.7%</td>
          <td>77.6%</td>
          <td>61.5%</td>
          <td>64.7%</td>
          <td>55.0%</td>
          <td>74.0%</td>
          <td class="second-highest">31.3%</td>
          <td>68.7%</td>
          <td>53.5%</td>
          <td>68.0%</td>
          <td>64.9%</td>
        </tr>
        <tr>
          <td>GPT-4o</td>
          <td>-</td>
          <td class="highest">83.3%</td>
          <td class="second-highest">81.6%</td>
          <td>60.5%</td>
          <td class="second-highest">74.7%</td>
          <td>51.5%</td>
          <td>68.0%</td>
          <td>30.7%</td>
          <td>60.7%</td>
          <td>59.0%</td>
          <td>75.3%</td>
          <td>66.4%</td>
        </tr>
        <tr>
          <td>Gemini-2.5 Pro</td>
          <td>-</td>
          <td class="second-highest">83.0%</td>
          <td class="highest">91.6%</td>
          <td>64.5%</td>
          <td>74.0%</td>
          <td class="highest">70.0%</td>
          <td class="highest">84.4%</td>
          <td class="highest">48.7%</td>
          <td>74.0%</td>
          <td>56.5%</td>
          <td>72.0%</td>
          <td class="highest">73.7%</td>
        </tr>
        <tr>
            <th colspan="13" class="section-header has-text-centered">Small-scale MLLMs</th>
        </tr>
        <tr>
          <td>LLaVA-OneVision</td>
          <td>7B</td>
          <td>42.0%</td>
          <td>50.0%</td>
          <td>49.0%</td>
          <td>54.0%</td>
          <td>41.0%</td>
          <td>46.4%</td>
          <td>20.0%</td>
          <td>45.3%</td>
          <td>52.0%</td>
          <td>60.0%</td>
          <td>46.0%</td>
        </tr>
        <tr>
          <td>VideoChat-Flash</td>
          <td>7B</td>
          <td>36.7%</td>
          <td>48.0%</td>
          <td>60.0%</td>
          <td>60.0%</td>
          <td>49.0%</td>
          <td>46.0%</td>
          <td>19.3%</td>
          <td>50.0%</td>
          <td>54.0%</td>
          <td>60.7%</td>
          <td>47.8%</td>
        </tr>
        <tr>
          <td>Slow-fast MLLM</td>
          <td>7B</td>
          <td>46.0%</td>
          <td>54.8%</td>
          <td>52.0%</td>
          <td>60.0%</td>
          <td>47.0%</td>
          <td>48.0%</td>
          <td>20.0%</td>
          <td>43.3%</td>
          <td>48.5%</td>
          <td>54.0%</td>
          <td>47.8%</td>
        </tr>
        <tr>
          <td>VITA-1.5</td>
          <td>7B</td>
          <td>49.0%</td>
          <td>58.4%</td>
          <td>43.0%</td>
          <td>61.3%</td>
          <td>49.0%</td>
          <td>53.2%</td>
          <td>20.0%</td>
          <td>51.3%</td>
          <td>47.0%</td>
          <td>58.7%</td>
          <td>49.5%</td>
        </tr>
        <tr>
          <td>Oryx-1.5</td>
          <td>7B</td>
          <td>51.7%</td>
          <td>54.0%</td>
          <td>50.5%</td>
          <td>54.7%</td>
          <td>44.5%</td>
          <td>52.8%</td>
          <td>23.3%</td>
          <td>48.7%</td>
          <td>47.0%</td>
          <td>64.0%</td>
          <td>49.6%</td>
        </tr>
        <tr>
          <td>LLaVA-Video</td>
          <td>7B</td>
          <td>47.0%</td>
          <td>59.2%</td>
          <td>61.0%</td>
          <td>68.7%</td>
          <td>48.5%</td>
          <td>50.0%</td>
          <td>21.3%</td>
          <td>47.3%</td>
          <td>56.5%</td>
          <td>68.7%</td>
          <td>52.8%</td>
        </tr>
        <tr>
          <td>VideoLLaMA 3</td>
          <td>7B</td>
          <td>47.3%</td>
          <td>57.6%</td>
          <td class="highest">68.0%</td>
          <td>64.7%</td>
          <td>50.0%</td>
          <td>54.0%</td>
          <td>21.3%</td>
          <td>48.7%</td>
          <td>55.0%</td>
          <td>67.3%</td>
          <td>53.5%</td>
        </tr>
        <tr>
          <td>Qwen2.5-VL</td>
          <td>7B</td>
          <td>70.3%</td>
          <td>70.0%</td>
          <td>58.0%</td>
          <td>68.7%</td>
          <td>48.5%</td>
          <td>66.4%</td>
          <td>17.3%</td>
          <td>49.3%</td>
          <td>53.0%</td>
          <td>71.3%</td>
          <td>59.1%</td>
        </tr>
        <tr>
          <td>InternVL3</td>
          <td>8B</td>
          <td>61.3%</td>
          <td>72.0%</td>
          <td>60.0%</td>
          <td>69.3%</td>
          <td>56.5%</td>
          <td>62.4%</td>
          <td>23.3%</td>
          <td>57.3%</td>
          <td>55.0%</td>
          <td>71.3%</td>
          <td>59.8%</td>
        </tr>
         <tr>
            <th colspan="13" class="section-header has-text-centered">Middle-scale MLLMs</th>
        </tr>
        <tr>
          <td>Oryx-1.5</td>
          <td>32B</td>
          <td>50.3%</td>
          <td>60.0%</td>
          <td>63.5%</td>
          <td>62.7%</td>
          <td>46.0%</td>
          <td>60.4%</td>
          <td>21.3%</td>
          <td>54.7%</td>
          <td class="second-highest">61.0%</td>
          <td>68.0%</td>
          <td>55.2%</td>
        </tr>
        <tr>
          <td>Kimi-VL</td>
          <td>16B</td>
          <td>54.7%</td>
          <td>66.4%</td>
          <td>59.0%</td>
          <td>62.7%</td>
          <td>48.0%</td>
          <td>57.6%</td>
          <td>23.3%</td>
          <td>56.7%</td>
          <td>57.5%</td>
          <td>71.3%</td>
          <td>56.2%</td>
        </tr>
        <tr>
          <td>Qwen2.5-VL</td>
          <td>32B</td>
          <td>58.3%</td>
          <td>77.2%</td>
          <td>62.5%</td>
          <td>68.7%</td>
          <td>52.0%</td>
          <td>70.4%</td>
          <td>22.7%</td>
          <td>68.7%</td>
          <td>54.5%</td>
          <td>65.3%</td>
          <td>61.0%</td>
        </tr>
        <tr>
          <td>InternVL3</td>
          <td>38B</td>
          <td>67.0%</td>
          <td>76.8%</td>
          <td>65.0%</td>
          <td class="highest">76.0%</td>
          <td>61.0%</td>
          <td>69.6%</td>
          <td>24.7%</td>
          <td class="second-highest">76.0%</td>
          <td class="highest">61.5%</td>
          <td class="second-highest">76.7%</td>
          <td>66.1%</td>
        </tr>
         <tr>
          <th colspan="13" class="section-header">Large-scale MLLMs</th>
        </tr>
        <tr>
          <td>InternVL3</td>
          <td>78B</td>
          <td>70.0%</td>
          <td>77.6%</td>
          <td class="second-highest">67.5%</td>
          <td class="highest">76.0%</td>
          <td class="second-highest">65.5%</td>
          <td>71.6%</td>
          <td>24.7%</td>
          <td class="highest">77.3%</td>
          <td>57.0%</td>
          <td>75.3%</td>
          <td>67.2%</td>
        </tr>
        <tr>
          <td>Qwen2.5-VL</td>
          <td>72B</td>
          <td>80.7%</td>
          <td>80.0%</td>
          <td>65.0%</td>
          <td>74.0%</td>
          <td>56.5%</td>
          <td class="second-highest">79.6%</td>
          <td>26.7%</td>
          <td>74.7%</td>
          <td>57.0%</td>
          <td class="highest">78.7%</td>
          <td class="second-highest">69.0%</td>
        </tr>
      </tbody>
    </table>
    </div>
</section>

<!-- Visualization -->
<section class="section is-small-padding">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="section is-80-percent">
        <h2 class="title is-3">Benchmark Statistics</h2>

        <figure class="is-tight-figure">
          <img src="static/images/statistics.png" alt="Teaser Image" style="max-width: 100%; height: auto;">
        </figure>

        <!-- 修改后的文字部分 -->
        <p style="text-align: left; white-space: normal; max-width: 95%; margin: 0 auto;">
          <strong>Overview of MME-VideoOCR Statistics</strong>. The videos in MME-VideoOCR covers 9 major scenario categories comprising 44 specific scene types, offering fine-grained coverage of diverse video contexts. The benchmark features a balanced distribution of video durations and sources, with a significant portion of the videos newly collected from public resources or manually curated.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Visualization -->

<!-- Visualization -->
<section class="section is-small-padding">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="section is-80-percent">
        <h2 class="title is-3">Construction Process</h2>

        <figure class="is-tight-figure">
          <img src="static/images/construction.png" alt="Teaser Image" style="max-width: 100%; height: auto;">
        </figure>

        <!-- 修改后的文字部分 -->
        <p style="text-align: left; white-space: normal; max-width: 95%; margin: 0 auto;">
          <strong>Overview of the MME-VideoOCR construction process</strong>. Video filtering ensures sufficient visual dynamics and meaningful textual content. Manual annotation provides high-quality QA pairs, and expert verification further enhances sample reliability and mitigates potential biases.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Visualization -->

<!-- Analysis Toggle Button -->
<div class="has-text-centered" style="margin-bottom: 1rem;">
  <button class="button is-info is-light is-large" onclick="toggleAnalysis()">
    <span class="is-size-3">Show Analysis</span>
  </button>
</div>

<!-- Analysis Section -->
<section id="analysis-section" class="section is-small-padding" style="display: none;">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-12-desktop">
        <h2 class="title is-3 has-text-centered">Analysis</h2>
        <div style="border: 1px solid #ddd; border-radius: 12px; padding: 2rem; box-shadow: 0 4px 12px rgba(0,0,0,0.05); background-color: #fafafa;">

          <p>
            <strong>Model Performance and Benchmark Discriminative Power.</strong><br>
            We evaluate 18 state-of-the-art MLLMs, including both open-source models (7B–78B) and closed-source models such as GPT-4o and Gemini-2.5 Pro. The results highlight the strong discriminative capability of the MME-VideoOCR benchmark. The accuracy gap between the best-performing model (Gemini-2.5 Pro, 73.7%) and the lowest (LLaVA-OneVision 7B, 46.0%) underscores substantial performance variation across models. In terms of task difficulty, performance on tasks such as Cross-Frame Text Understanding and Text-Based Video Understanding remains generally low, with most models scoring below 60%, reflecting the challenges of temporal integration and semantic reasoning in video OCR.
          </p><br><br>


            
            <p>
            <strong>Impact of Resolution and Frame Count.</strong><br>
            <figure class="is-tight-figure">
            <img src="static/images/frame_num_and_res.png" style="max-width: 100%; height: auto;">
            </figure>
            Our experiments demonstrate that both higher input resolution and a greater number of frames significantly improve model performance on video OCR tasks. Increasing the resolution of each frame consistently enhances accuracy across all tested models. Similarly, extending the number of input frames generally benefits performance. However, models such as Qwen2.5-VL and InternVL3 show a slight performance drop when the frame count increases from 32 to 64, suggesting that excessively long contexts may hinder attention allocation and degrade performance.
            </p><br><br>

          <p>
            <strong>Textual Information Utilization.</strong><br>
            In tasks like Subtitle-Based Video Understanding, where relevant information is concentrated in a small number of frames, most models perform well. This indicates that current MLLMs are capable of leveraging embedded textual cues for effective semantic comprehension when the temporal scope is limited.
          </p><br><br>

          <p>
            <strong>Limitations in Temporal Integration.</strong><br>
            Models exhibit significant limitations in tasks that require cross-frame reasoning. Most achieve only around 20% accuracy on Cross-Frame Text Understanding, while all models fail entirely on Trajectory Recognition, with an accuracy of 0%. Additionally, the majority score below 35% on Scrambled Recognition. These results indicate a consistent weakness in integrating temporally dispersed information, as current models tend to rely on localized context rather than constructing a comprehensive temporal understanding.
          </p><br><br>

          <p>
            <strong>Language Prior Bias.</strong><br>
            <figure class="is-tight-figure">
            <img src="static/images/bias_fig.png" style="max-width: 100%; height: auto;">
            </figure>
            MLLMs exhibit a strong tendency to prioritize language priors over visual fidelity. They often normalize misspelled yet visually clear text into contextually likely words, leading to systematic errors. This behavior indicates a reliance on linguistic plausibility rather than accurate visual transcription, undermining performance in OCR tasks where character-level precision is critical.
          </p><br><br>

          <p>
            <strong>Key Takeaways for Model Optimization.</strong><br>
            Overall, the results suggest that robust video OCR requires not only high-resolution visual input and extended temporal context, but also architectural improvements for long-range integration and a better balance between visual evidence and linguistic priors. These remain critical directions for the next generation of MLLMs.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Analysis Section -->

<!-- Toggle Script -->
<script>
  function toggleAnalysis() {
    const section = document.getElementById("analysis-section");
    const button = event.currentTarget;
    const isVisible = section.style.display === "block";
    section.style.display = isVisible ? "none" : "block";
    button.textContent = isVisible ? "Show Analysis" : "Hide Analysis";
  }
</script>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->

</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
